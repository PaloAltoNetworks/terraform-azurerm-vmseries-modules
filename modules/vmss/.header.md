# Palo Alto Networks VMSS Module for Azure

A terraform module for VMSS VM-Series firewalls in Azure.

**NOTE** \
Due to [lack of proper method of running health probes](./main.tf#L21-54) against Pan-OS based VMs running in a Scale Set, the `upgrade_mode` property is hardcoded to `Manual`. For this mode to actually work the `roll_instances_when_required` provider feature has to be also configured and set to `false`. Unfortunately this cannot be set in the `vmss` module, it has to be specified in the **root** module.

Therefore, when using this module please add the following `provider` block to your code:

```hcl
provider "azurerm" {
  features {
    virtual_machine_scale_set {
      roll_instances_when_required = false
    }
  }
}
```

## Usage

```hcl
module "vmss" {
  source = "PaloAltoNetworks/vmseries-modules/azurerm//modules/vmss"

  location                  = "Australia Central"
  name_prefix               = "pan"
  password                  = "your-password"
  subnet_mgmt               = azurerm_subnet.subnet_mgmt
  subnet_private            = azurerm_subnet.subnet_private
  subnet_public             = module.networks.subnet_public
  bootstrap_storage_account = module.panorama.bootstrap_storage_account
  bootstrap_share_name      = "inboundsharename"
  vhd_container             = "vhd-storage-container-id"
  lb_backend_pool_id        = "private-backend-pool-id"
}
```

## SOme info about rolling upgrades

Allowing upgrade_mode = "Rolling" would be actually a big architectural change. First of all
Error: `health_probe_id` must be set or a health extension must be specified when `upgrade_mode` is set to "Rolling"
VM-Series do not have a health extension
Having health_probe_id, as visible in the next error message below, Azure requires the first NIC to be
the load-balanced one. Azure complains about "inbound-nic-fw-mgmt", which in that case was the primary IP config
of the first NIC

```
Error: Error creating Linux Virtual Machine Scale Set "inbound-VMSS" (Resource Group "example-vmss-inbound")
compute.VirtualMachineScaleSetsClient#CreateOrUpdate: Failure sending request: StatusCode=0 -- Original Error

Code="CannotUseHealthProbeWithoutLoadBalancing"

Message="VM scale set /subscriptions/d47f1af8-9795-4e86-bbce-da72cfd0f8ec/resourceGroups/EXAMPLE-VMSS-INBOUND/providers/Microsoft.Compute/virtualMachineScaleSets/inbound-VMSS cannot use probe /subscriptions/d47f1af8-9795-4e86-bbce-da72cfd0f8ec/resourceGroups/example-vmss-inbound/providers/Microsoft.Network/loadBalancers/inbound-public-elb/probes/inbound-public-elb as a HealthProbe because primary IP configuration inbound-nic-fw-mgmt of the scale set does not use load balancing. LoadBalancerBackendAddressPools property of the IP configuration must reference backend address pool of the load balancer that contains the probe."
Details=[]
│
│   with module.inbound_scale_set.azurerm_linux_virtual_machine_scale_set.this
│   on ../../modules/vmss/main.tf line 1, in resource "azurerm_linux_virtual_machine_scale_set" "this"
│    1: resource "azurerm_linux_virtual_machine_scale_set" "this" {

```

Hence mgmt-interface-swap seems to be required on VM-Series, which would need a major overhaul of the
subnet-related inputs. Without the mgmt-interface-swap, it seems impossible to have upgrade_mode = "Rolling"
The phony LB on a management network does not seem a viable solution. For now Azure does not support two internal
load balancers per VM. Also, health checking HTTP/SSH on management port would wrongly consider that unconfigured
VM-Series is good to use. Unconfigured VM-Series still shows HTTP/SSH on the management interface. This does not
happen when checking a dataplane interface, because the data only shows HTTP/SSH after the initial commit applies
a specific management profile
Also the inbound vmss would have the ethernet1/1 public and ethernet1/2 private, but outbound vmss would have
the ethernet1/1 private and ethernet1/2 public. That ensures the respective LB health probe works on ethernet1/1
which is the first NIC
The automatic_instance_repair also suffers from exactly the same problem
"Automatic repairs not supported for this Virtual Machine Scale Set because a health probe or health extension was not provided."

## Custom Metrics

Firewalls can publish custom metrics (for example `panSessionUtilization`) to Azure Application Insights to improve the autoscaling.
This however requires a manual initialization: copy the outputs `metrics_instrumentation_key` and paste it into your
PAN-OS webUI -> Device -> VM-Series -> Azure. This module automatically
completes solely the Step 1 of the [official procedure](https://docs.paloaltonetworks.com/vm-series/10-0/vm-series-deployment/set-up-the-vm-series-firewall-on-azure/enable-azure-application-insights-on-the-vm-series-firewall.html).

If you manage the configuration from Panorama, this can be done in the same place, however the PAN-OS `VM-Series plugin` needs to be installed **on both** Panorama and VM-Series.

The metrics gathered within a single Azure Application Insights instance provided by the module, cannot be split to obtain
back a result for solely a single firewall. Thus for example if three firewalls use the same Instrumentation Key and report
their respective session utilizations as 90%, 20%, 10%, it is possible to see in Azure the average of 40%, the sum of 120%, the max of 90%, but it is *not possible* to know which of the firewalls reported the 90% utilization.
